# Plotting utilities

Plotting utilities for visualizing conformal prediction results and STL (Signal Temporal Logic) verification data from running the power grid simulation.

## Different ways to use the plotting script

If the experience has several episodes we might not be interested in plotting all of them.

For this we set the `AUTO_GEN_PLOTS = False` option in [config.py](../config.py) and we use the [plotting_script.py](../plotting_script.py) manually.

Ways to use:

```sh
python plotting_script <FOLDER_CONTAINING_CSVS> # Plots inside the same folder

# or

python plotting_script <FOLDER_CONTAINING_CSVS> <OPTIONAL_OUTPUT_FOLDER> # Plots inside the folder specified in OPTIONAL_OUTPUT_FOLDER
```

Some examples:

```sh
python plotting_script.py RESULTS/MY_EXP/ OPTIONAL_OUTPUT_FOLDER

# or

python plotting_script.py RESULTS/MY_EXP/alpha_0.1/ OPTIONAL_OUTPUT_FOLDER

#or

python plotting_script.py RESULTS/MY_EXP/alpha_0.1/episode_13/ OPTIONAL_OUTPUT_FOLDER

# or

python plotting_script.py RESULTS/MY_EXP/alpha_0.1/aggregated_csvs/ OPTIONAL_OUTPUT_FOLDER
```

## Modules

### `plotting_config.py`

Central configuration for all plots. Contains global font settings, layout parameters, color mappings for models and STL rules, and display name mappings (e.g., "vanilla" can become "My Vanilla Model").
There are functions for applying consistent styling across all plots.

These parameters, for example, are customizable:

- `MODELS_TO_PLOT` / `RULES_TO_PLOT` - which models or rules to include (`None` for all)
- `COLORS` - color mapping for models and rules
- `DISPLAY_NAMES` - The names that we display, for more pleasing figures
- `FontConfig` / `LayoutConfig` - Configures font size/weight and layout config

### `data.py`

Loading the csv files and extracting the data, such as names, line names, STL rule names from config files, etc..
Allows to filter based on `MODELS_TO_PLOT` and `RULES_TO_PLOT` settings. This allows to only plot a subset of rules or models from the csv files.

### `start_plots.py`

Figures out which plots to create based on the provided .csv files and runs plotting tasks in parallel using `ProcessPoolExecutor`. Also handles the logic for collecting data across multiple alpha directories for multi-alpha comparisons.

## The remaining modules implement each type of plot

### `conformal_plots.py`

Generates prediction interval plots:

- Scatter plots that show the evolution of the episodes, including actual vs forecast values, the conformal interval, uncovered points (in red), etc.
- Plots grid with all selected conformal models, or folder with each conformal model isolated.

### `comparison_plots.py`

The comparison bar charts:

- The first page compares coverage.
- The second page compares average with.
- The third page compares coverage (in action-influenced timesteps)

Also the horizon plots:

- Coverage by forecast horizon
- Width by forecast horizon

### `classification_plots.py`

Experimental plots with f1 score, false alarms (fa), overlooked violations (miss), confusion matrix and
the ROC-style plot that uses false alarms and overlooked violations.

### `stl_plots.py`

STL plots in which the actual values (the real values of rho) are collered green or red depending on how the entire trajectory was classified.
If a trajectory was classified as safe, it plots all the points of that trajectory in green, otherwise in red.
The results are perfect if only red points are above the config.RHO_SAFETY_THRESHOLD line, and if only green points are below that line.

### `conf_plots_like_stl.py`

Also experimental plots. These plots mimic the ones generated by stl_plots.py, but for the conformal models.
A trajectory is considered unsafe (for the conformal models) if any timestep in that trajectory has a conformal
interval upper bound that exceeds the config.RHO_SAFETY_THRESHOLD.
The style is the same as in stl_plots.py, but we include the prediction interval, so it is visually obvious it came
from the conformal models.

### `multi_alpha_plots.py`

The framework can be run with multiple alpha values (significance levels) set (see config.py in the root folder).
The multi alpha plots generated by this file result in a single pdf
which shows f1 scores, false alarms, and overlooked violations by alpha.
It also shows the confusion matrices and roc-like plots similar to the ``classification_plots.py``

Generates comparison plots across multiple alpha (significance level) values, showing how metrics and ROC curves change as alpha varies.
